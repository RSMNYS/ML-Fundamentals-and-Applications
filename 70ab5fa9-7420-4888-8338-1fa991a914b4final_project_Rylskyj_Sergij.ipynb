{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eQCf-kATGgJL",
        "outputId": "32d45507-8616-44e7-98bb-f7ac00f661b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.7)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.13.1)\n",
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.10/dist-packages (2.6.4)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.4)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.1)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost\n",
        "!pip install lightgbm\n",
        "!pip install category_encoders\n",
        "!pip install imbalanced-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "# Data manipulation and preprocessing\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import SimpleImputer, IterativeImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, PolynomialFeatures\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, precision_recall_curve\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "# Machine learning models\n",
        "from catboost import CatBoostClassifier\n",
        "import lightgbm as lgbm\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Handling imbalanced data\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Additional tools\n",
        "import category_encoders as ce\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "866vS5VSGqyj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_data = pd.read_csv(\"/content/final_proj_data.csv\")\n",
        "test_data = pd.read_csv('/content/final_proj_test.csv')\n",
        "\n",
        "# Print initial data info\n",
        "print(\"Training Data Shape:\", train_data.shape)\n",
        "print(\"\\nTest Data Shape:\", test_data.shape)\n",
        "print(\"\\nTraining Data Info:\")\n",
        "print(train_data.info())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtyAhwZBG08D",
        "outputId": "d2b4188b-ff62-48fc-cbfe-9f6cab97fb58"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Shape: (10000, 231)\n",
            "\n",
            "Test Data Shape: (2500, 230)\n",
            "\n",
            "Training Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Columns: 231 entries, Var1 to y\n",
            "dtypes: float64(191), int64(2), object(38)\n",
            "memory usage: 17.6+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_and_select_numerical_features(X, y, correlation_threshold=0.8):\n",
        "    \"\"\"\n",
        "    Analyze correlations and evaluate feature importance before removal\n",
        "    \"\"\"\n",
        "    # Get numeric features\n",
        "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = X[numeric_features].corr().abs()\n",
        "\n",
        "    # Find highly correlated pairs\n",
        "    high_corr_pairs = []\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i+1, len(corr_matrix.columns)):\n",
        "            if corr_matrix.iloc[i, j] > correlation_threshold:\n",
        "                feat1, feat2 = corr_matrix.columns[i], corr_matrix.columns[j]\n",
        "                corr_val = corr_matrix.iloc[i, j]\n",
        "                high_corr_pairs.append((feat1, feat2, corr_val))\n",
        "\n",
        "    if high_corr_pairs:\n",
        "        print(\"\\nHighly correlated feature pairs:\")\n",
        "        for f1, f2, corr in high_corr_pairs:\n",
        "            print(f\"{f1} - {f2}: {corr:.3f}\")\n",
        "\n",
        "    # Initial feature importance using CatBoost\n",
        "    print(\"\\nCalculating initial feature importance...\")\n",
        "    initial_model = CatBoostClassifier(\n",
        "        iterations=100,\n",
        "        learning_rate=0.1,\n",
        "        depth=6,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Fill NaN values for initial importance calculation\n",
        "    X_filled = X[numeric_features].fillna(X[numeric_features].median())\n",
        "    initial_model.fit(X_filled, y)\n",
        "\n",
        "    # Get feature importance\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': numeric_features,\n",
        "        'importance': initial_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(\"\\nTop 10 most important features:\")\n",
        "    print(importance_df.head(10))\n",
        "\n",
        "    # Strategy: Keep both correlated features if both are in top important features\n",
        "    top_important = set(importance_df.head(20)['feature'])\n",
        "    features_to_remove = set()\n",
        "\n",
        "    for feat1, feat2, corr in high_corr_pairs:\n",
        "        # If both features are important, keep them\n",
        "        if feat1 in top_important and feat2 in top_important:\n",
        "            continue\n",
        "        # If one is important, remove the other\n",
        "        elif feat1 in top_important:\n",
        "            features_to_remove.add(feat2)\n",
        "        elif feat2 in top_important:\n",
        "            features_to_remove.add(feat1)\n",
        "        # If neither is important, remove the one with lower importance\n",
        "        else:\n",
        "            feat1_imp = importance_df[importance_df['feature'] == feat1]['importance'].iloc[0]\n",
        "            feat2_imp = importance_df[importance_df['feature'] == feat2]['importance'].iloc[0]\n",
        "            if feat1_imp < feat2_imp:\n",
        "                features_to_remove.add(feat1)\n",
        "            else:\n",
        "                features_to_remove.add(feat2)\n",
        "\n",
        "    selected_features = [f for f in numeric_features if f not in features_to_remove]\n",
        "\n",
        "    print(f\"\\nSelected {len(selected_features)} features\")\n",
        "    print(f\"Removed {len(features_to_remove)} features\")\n",
        "\n",
        "    return selected_features, features_to_remove, importance_df\n"
      ],
      "metadata": {
        "id": "jc0mLNCiuFsa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_cat_feature_types(data):\n",
        "    \"\"\"Identify numeric and categorical features\"\"\"\n",
        "    categorical_features = data.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Split categorical features based on cardinality\n",
        "    low_cardinality = []\n",
        "    high_cardinality = []\n",
        "    for col in categorical_features:\n",
        "        if data[col].nunique() <= 10:\n",
        "            low_cardinality.append(col)\n",
        "        else:\n",
        "            high_cardinality.append(col)\n",
        "\n",
        "    return low_cardinality, high_cardinality"
      ],
      "metadata": {
        "id": "exGGgg29PJMu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with 100% missing values\n",
        "missing_percentage = train_data.isnull().mean() * 100\n",
        "columns_to_drop = missing_percentage[missing_percentage == 100].index\n",
        "train_data_cleaned = train_data.drop(columns=columns_to_drop)"
      ],
      "metadata": {
        "id": "8kX4YYZ3R4MG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "def set_all_seeds(seed=42):\n",
        "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_all_seeds()"
      ],
      "metadata": {
        "id": "wkE9688LXb3-"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data\n",
        "X = train_data_cleaned.drop('y', axis=1)\n",
        "y = train_data_cleaned['y']\n",
        "\n",
        "# Analyze features\n",
        "selected_features, features_to_remove, importance_df = analyze_and_select_numerical_features(X, y)\n",
        "low_cardinality, high_cardinality = identify_cat_feature_types(train_data_cleaned)\n",
        "\n",
        "print(\"\\nFeature types found:\")\n",
        "print(f\"Numeric features ({len(selected_features)}): {selected_features}\")\n",
        "print(f\"Low cardinality features ({len(low_cardinality)}): {low_cardinality}\")\n",
        "print(f\"High cardinality features ({len(high_cardinality)}): {high_cardinality}\")\n",
        "\n",
        "# Split data with stratification\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Calculate weights for balanced classes\n",
        "pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "print(f\"\\nPositive class weight: {pos_weight:.2f}\")\n",
        "\n",
        "# Create preprocessing pipeline optimized for telecom data\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        # Numeric features - use robust scaling due to possible outliers\n",
        "        ('num', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='median')),  # Changed to median\n",
        "            ('scaler', RobustScaler())\n",
        "        ]), selected_features),\n",
        "\n",
        "        # Low cardinality categorical features\n",
        "        ('low_card', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "        ]), low_cardinality),\n",
        "\n",
        "        # High cardinality categorical features\n",
        "        ('high_card', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "            ('encoder', ce.CatBoostEncoder(sigma=0.05))\n",
        "        ]), high_cardinality)\n",
        "    ],\n",
        "    remainder='drop',\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# Process features\n",
        "print(\"\\nProcessing features...\")\n",
        "X_train_processed = preprocessor.fit_transform(X_train, y_train)\n",
        "X_val_processed = preprocessor.transform(X_val)\n",
        "\n",
        "print(f\"\\nProcessed training data shape: {X_train_processed.shape}\")\n",
        "print(f\"Processed validation data shape: {X_val_processed.shape}\")\n",
        "\n",
        "# Create model optimized for telecom churn\n",
        "model = CatBoostClassifier(\n",
        "        iterations=2000,\n",
        "        learning_rate=0.02,\n",
        "        depth=6,\n",
        "        l2_leaf_reg=3,\n",
        "        min_data_in_leaf=10,\n",
        "        random_strength=0,\n",
        "        scale_pos_weight=pos_weight,\n",
        "        random_state=42,\n",
        "        verbose=100,\n",
        "        loss_function='Logloss',\n",
        "        eval_metric='AUC',\n",
        "        bootstrap_type='No',\n",
        "        leaf_estimation_method='Newton',\n",
        "        max_bin=256\n",
        "    )\n",
        "\n",
        "print(\"\\nTraining model...\")\n",
        "model.fit(\n",
        "    X_train_processed,\n",
        "    y_train,\n",
        "    eval_set=[(X_val_processed, y_val)],\n",
        "    early_stopping_rounds=50,\n",
        "    verbose=100\n",
        ")\n",
        "\n",
        "#Evaluate on validation set\n",
        "y_val_pred = model.predict(X_val_processed)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, y_val_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCe2XRe2cqbk",
        "outputId": "7cdb4d78-4c01-4558-e08f-4e6c9a32adff"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Highly correlated feature pairs:\n",
            "Var1 - Var9: 0.926\n",
            "Var1 - Var41: 0.872\n",
            "Var1 - Var63: 0.873\n",
            "Var1 - Var66: 0.926\n",
            "Var1 - Var77: 0.904\n",
            "Var1 - Var129: 0.837\n",
            "Var1 - Var156: 0.926\n",
            "Var1 - Var187: 0.971\n",
            "Var9 - Var63: 0.990\n",
            "Var9 - Var66: 1.000\n",
            "Var9 - Var77: 0.992\n",
            "Var9 - Var121: 0.955\n",
            "Var9 - Var129: 0.977\n",
            "Var9 - Var156: 1.000\n",
            "Var9 - Var187: 0.974\n",
            "Var12 - Var62: 0.987\n",
            "Var12 - Var63: 0.813\n",
            "Var12 - Var121: 0.890\n",
            "Var12 - Var129: 0.840\n",
            "Var17 - Var18: 0.945\n",
            "Var17 - Var88: 0.989\n",
            "Var17 - Var99: 0.975\n",
            "Var17 - Var101: 0.960\n",
            "Var17 - Var127: 0.972\n",
            "Var17 - Var128: 0.989\n",
            "Var17 - Var145: 0.986\n",
            "Var17 - Var158: 0.893\n",
            "Var17 - Var164: 0.937\n",
            "Var17 - Var174: 0.970\n",
            "Var17 - Var179: 0.951\n",
            "Var18 - Var88: 0.968\n",
            "Var18 - Var99: 0.960\n",
            "Var18 - Var101: 0.973\n",
            "Var18 - Var127: 0.984\n",
            "Var18 - Var128: 0.968\n",
            "Var18 - Var145: 0.968\n",
            "Var18 - Var158: 0.909\n",
            "Var18 - Var164: 0.935\n",
            "Var18 - Var174: 0.946\n",
            "Var18 - Var179: 0.938\n",
            "Var21 - Var22: 1.000\n",
            "Var21 - Var25: 0.821\n",
            "Var21 - Var83: 0.814\n",
            "Var21 - Var109: 0.805\n",
            "Var21 - Var112: 0.824\n",
            "Var21 - Var119: 0.810\n",
            "Var21 - Var123: 0.858\n",
            "Var21 - Var160: 0.909\n",
            "Var22 - Var25: 0.822\n",
            "Var22 - Var83: 0.814\n",
            "Var22 - Var109: 0.805\n",
            "Var22 - Var112: 0.824\n",
            "Var22 - Var119: 0.810\n",
            "Var22 - Var123: 0.858\n",
            "Var22 - Var160: 0.909\n",
            "Var23 - Var60: 0.982\n",
            "Var23 - Var61: 0.991\n",
            "Var23 - Var71: 0.969\n",
            "Var23 - Var91: 0.969\n",
            "Var23 - Var107: 0.801\n",
            "Var23 - Var120: 0.953\n",
            "Var23 - Var148: 0.968\n",
            "Var23 - Var151: 0.984\n",
            "Var23 - Var157: 0.984\n",
            "Var23 - Var166: 0.952\n",
            "Var30 - Var186: 0.911\n",
            "Var40 - Var115: 0.926\n",
            "Var41 - Var187: 0.816\n",
            "Var46 - Var68: 0.900\n",
            "Var46 - Var104: 0.863\n",
            "Var46 - Var105: 0.863\n",
            "Var60 - Var61: 0.990\n",
            "Var60 - Var71: 0.984\n",
            "Var60 - Var91: 0.984\n",
            "Var60 - Var107: 0.822\n",
            "Var60 - Var120: 0.969\n",
            "Var60 - Var148: 0.984\n",
            "Var60 - Var151: 0.981\n",
            "Var60 - Var157: 0.993\n",
            "Var60 - Var166: 0.967\n",
            "Var61 - Var71: 0.979\n",
            "Var61 - Var91: 0.979\n",
            "Var61 - Var107: 0.834\n",
            "Var61 - Var120: 0.968\n",
            "Var61 - Var148: 0.979\n",
            "Var61 - Var151: 0.985\n",
            "Var61 - Var157: 0.993\n",
            "Var61 - Var166: 0.955\n",
            "Var62 - Var121: 0.870\n",
            "Var62 - Var129: 0.818\n",
            "Var63 - Var66: 0.990\n",
            "Var63 - Var77: 0.989\n",
            "Var63 - Var121: 0.986\n",
            "Var63 - Var129: 0.992\n",
            "Var63 - Var156: 0.990\n",
            "Var63 - Var187: 0.935\n",
            "Var66 - Var77: 0.992\n",
            "Var66 - Var121: 0.955\n",
            "Var66 - Var129: 0.977\n",
            "Var66 - Var156: 1.000\n",
            "Var66 - Var187: 0.974\n",
            "Var68 - Var104: 0.965\n",
            "Var68 - Var105: 0.965\n",
            "Var68 - Var115: 0.823\n",
            "Var71 - Var91: 1.000\n",
            "Var71 - Var107: 0.864\n",
            "Var71 - Var120: 0.986\n",
            "Var71 - Var148: 1.000\n",
            "Var71 - Var151: 0.959\n",
            "Var71 - Var157: 0.988\n",
            "Var71 - Var166: 0.990\n",
            "Var75 - Var159: 0.854\n",
            "Var77 - Var121: 0.956\n",
            "Var77 - Var129: 0.975\n",
            "Var77 - Var156: 0.992\n",
            "Var77 - Var187: 0.958\n",
            "Var83 - Var109: 0.813\n",
            "Var88 - Var99: 0.993\n",
            "Var88 - Var101: 0.980\n",
            "Var88 - Var127: 0.988\n",
            "Var88 - Var128: 1.000\n",
            "Var88 - Var145: 1.000\n",
            "Var88 - Var158: 0.902\n",
            "Var88 - Var164: 0.951\n",
            "Var88 - Var174: 0.990\n",
            "Var88 - Var179: 0.975\n",
            "Var91 - Var107: 0.864\n",
            "Var91 - Var120: 0.986\n",
            "Var91 - Var148: 1.000\n",
            "Var91 - Var151: 0.959\n",
            "Var91 - Var157: 0.988\n",
            "Var91 - Var166: 0.990\n",
            "Var99 - Var101: 0.962\n",
            "Var99 - Var127: 0.974\n",
            "Var99 - Var128: 0.993\n",
            "Var99 - Var145: 0.993\n",
            "Var99 - Var158: 0.888\n",
            "Var99 - Var164: 0.941\n",
            "Var99 - Var174: 0.972\n",
            "Var99 - Var179: 0.953\n",
            "Var101 - Var127: 0.992\n",
            "Var101 - Var128: 0.980\n",
            "Var101 - Var145: 0.979\n",
            "Var101 - Var158: 0.937\n",
            "Var101 - Var164: 0.958\n",
            "Var101 - Var174: 0.967\n",
            "Var101 - Var179: 0.977\n",
            "Var104 - Var105: 1.000\n",
            "Var104 - Var115: 0.860\n",
            "Var105 - Var115: 0.860\n",
            "Var107 - Var120: 0.849\n",
            "Var107 - Var148: 0.866\n",
            "Var107 - Var157: 0.877\n",
            "Var107 - Var166: 0.833\n",
            "Var109 - Var112: 0.855\n",
            "Var120 - Var148: 0.986\n",
            "Var120 - Var151: 0.945\n",
            "Var120 - Var157: 0.973\n",
            "Var120 - Var166: 0.966\n",
            "Var121 - Var129: 0.988\n",
            "Var121 - Var156: 0.955\n",
            "Var121 - Var187: 0.869\n",
            "Var127 - Var128: 0.988\n",
            "Var127 - Var145: 0.988\n",
            "Var127 - Var158: 0.921\n",
            "Var127 - Var164: 0.967\n",
            "Var127 - Var174: 0.975\n",
            "Var127 - Var179: 0.979\n",
            "Var128 - Var145: 1.000\n",
            "Var128 - Var158: 0.902\n",
            "Var128 - Var164: 0.951\n",
            "Var128 - Var174: 0.990\n",
            "Var128 - Var179: 0.975\n",
            "Var129 - Var156: 0.977\n",
            "Var129 - Var187: 0.908\n",
            "Var145 - Var158: 0.901\n",
            "Var145 - Var164: 0.951\n",
            "Var145 - Var174: 0.990\n",
            "Var145 - Var179: 0.975\n",
            "Var148 - Var151: 0.958\n",
            "Var148 - Var157: 0.988\n",
            "Var148 - Var166: 0.990\n",
            "Var151 - Var157: 0.974\n",
            "Var151 - Var166: 0.938\n",
            "Var156 - Var187: 0.974\n",
            "Var157 - Var166: 0.968\n",
            "Var158 - Var164: 0.919\n",
            "Var158 - Var174: 0.875\n",
            "Var158 - Var179: 0.883\n",
            "Var164 - Var174: 0.925\n",
            "Var164 - Var179: 0.929\n",
            "Var174 - Var179: 0.978\n",
            "Var177 - Var184: 0.821\n",
            "\n",
            "Calculating initial feature importance...\n",
            "\n",
            "Top 10 most important features:\n",
            "    feature  importance\n",
            "114  Var126   38.855366\n",
            "62    Var73   29.828022\n",
            "63    Var74    3.301612\n",
            "172  Var189    2.801541\n",
            "136  Var149    2.560733\n",
            "131  Var144    2.355094\n",
            "69    Var81    1.859125\n",
            "101  Var113    1.587245\n",
            "32    Var38    1.551534\n",
            "46    Var57    1.409927\n",
            "\n",
            "Selected 127 features\n",
            "Removed 47 features\n",
            "\n",
            "Feature types found:\n",
            "Numeric features (127): ['Var2', 'Var3', 'Var4', 'Var5', 'Var6', 'Var7', 'Var10', 'Var11', 'Var13', 'Var14', 'Var16', 'Var19', 'Var21', 'Var24', 'Var26', 'Var27', 'Var28', 'Var29', 'Var30', 'Var33', 'Var34', 'Var35', 'Var36', 'Var37', 'Var38', 'Var43', 'Var44', 'Var45', 'Var47', 'Var49', 'Var50', 'Var51', 'Var53', 'Var54', 'Var56', 'Var57', 'Var58', 'Var59', 'Var63', 'Var64', 'Var65', 'Var67', 'Var69', 'Var70', 'Var72', 'Var73', 'Var74', 'Var75', 'Var76', 'Var78', 'Var80', 'Var81', 'Var82', 'Var84', 'Var85', 'Var86', 'Var87', 'Var89', 'Var90', 'Var91', 'Var92', 'Var93', 'Var94', 'Var95', 'Var96', 'Var97', 'Var98', 'Var100', 'Var102', 'Var103', 'Var104', 'Var106', 'Var108', 'Var110', 'Var111', 'Var113', 'Var114', 'Var116', 'Var117', 'Var118', 'Var119', 'Var122', 'Var124', 'Var125', 'Var126', 'Var130', 'Var131', 'Var132', 'Var133', 'Var134', 'Var135', 'Var136', 'Var137', 'Var138', 'Var139', 'Var140', 'Var142', 'Var143', 'Var144', 'Var146', 'Var147', 'Var149', 'Var150', 'Var152', 'Var153', 'Var154', 'Var155', 'Var158', 'Var161', 'Var162', 'Var163', 'Var165', 'Var168', 'Var170', 'Var171', 'Var172', 'Var173', 'Var176', 'Var177', 'Var178', 'Var180', 'Var181', 'Var182', 'Var183', 'Var188', 'Var189', 'Var190']\n",
            "Low cardinality features (18): ['Var191', 'Var194', 'Var196', 'Var201', 'Var203', 'Var205', 'Var208', 'Var210', 'Var211', 'Var213', 'Var215', 'Var218', 'Var221', 'Var223', 'Var224', 'Var225', 'Var227', 'Var229']\n",
            "High cardinality features (20): ['Var192', 'Var193', 'Var195', 'Var197', 'Var198', 'Var199', 'Var200', 'Var202', 'Var204', 'Var206', 'Var207', 'Var212', 'Var214', 'Var216', 'Var217', 'Var219', 'Var220', 'Var222', 'Var226', 'Var228']\n",
            "\n",
            "Positive class weight: 6.66\n",
            "\n",
            "Processing features...\n",
            "\n",
            "Processed training data shape: (8000, 216)\n",
            "Processed validation data shape: (2000, 216)\n",
            "\n",
            "Training model...\n",
            "0:\ttest: 0.8922048\tbest: 0.8922048 (0)\ttotal: 23.2ms\tremaining: 46.3s\n",
            "100:\ttest: 0.9525821\tbest: 0.9525821 (100)\ttotal: 2.15s\tremaining: 40.5s\n",
            "200:\ttest: 0.9579712\tbest: 0.9579712 (200)\ttotal: 4.34s\tremaining: 38.8s\n",
            "300:\ttest: 0.9600929\tbest: 0.9600929 (300)\ttotal: 6.58s\tremaining: 37.2s\n",
            "400:\ttest: 0.9611835\tbest: 0.9612143 (399)\ttotal: 9.59s\tremaining: 38.2s\n",
            "500:\ttest: 0.9614721\tbest: 0.9615118 (494)\ttotal: 12.9s\tremaining: 38.7s\n",
            "Stopped by overfitting detector  (50 iterations wait)\n",
            "\n",
            "bestTest = 0.9615932881\n",
            "bestIteration = 510\n",
            "\n",
            "Shrink model to first 511 iterations.\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.88      0.93      1739\n",
            "           1       0.52      0.90      0.66       261\n",
            "\n",
            "    accuracy                           0.88      2000\n",
            "   macro avg       0.75      0.89      0.80      2000\n",
            "weighted avg       0.92      0.88      0.89      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process full training data\n",
        "print(\"\\nTraining final model on full dataset...\")\n",
        "X_full_processed = preprocessor.fit_transform(X, y)\n",
        "model.fit(X_full_processed, y, verbose=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_cDYJ13eX0W",
        "outputId": "120622c4-2a00-4dae-f837-8059d7c5e66d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training final model on full dataset...\n",
            "0:\ttotal: 27.6ms\tremaining: 55.2s\n",
            "100:\ttotal: 2.67s\tremaining: 50.3s\n",
            "200:\ttotal: 6.88s\tremaining: 1m 1s\n",
            "300:\ttotal: 9.22s\tremaining: 52.1s\n",
            "400:\ttotal: 11.7s\tremaining: 46.7s\n",
            "500:\ttotal: 14.2s\tremaining: 42.5s\n",
            "600:\ttotal: 16.8s\tremaining: 39.1s\n",
            "700:\ttotal: 21s\tremaining: 38.8s\n",
            "800:\ttotal: 23.3s\tremaining: 34.9s\n",
            "900:\ttotal: 25.7s\tremaining: 31.4s\n",
            "1000:\ttotal: 28.1s\tremaining: 28s\n",
            "1100:\ttotal: 30.9s\tremaining: 25.2s\n",
            "1200:\ttotal: 35.5s\tremaining: 23.6s\n",
            "1300:\ttotal: 38.4s\tremaining: 20.6s\n",
            "1400:\ttotal: 40.7s\tremaining: 17.4s\n",
            "1500:\ttotal: 43.2s\tremaining: 14.4s\n",
            "1600:\ttotal: 46.9s\tremaining: 11.7s\n",
            "1700:\ttotal: 49.9s\tremaining: 8.77s\n",
            "1800:\ttotal: 52.4s\tremaining: 5.79s\n",
            "1900:\ttotal: 54.7s\tremaining: 2.85s\n",
            "1999:\ttotal: 57.1s\tremaining: 0us\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x7e6524341b70>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_cleaned = test_data.drop(columns=columns_to_drop)\n",
        "\n",
        "X_test_processed = preprocessor.transform(test_data_cleaned)\n",
        "\n",
        "y_test_pred = model.predict(X_test_processed)\n",
        "\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'index': range(len(y_test_pred)),\n",
        "    'y': y_test_pred\n",
        "})\n",
        "\n",
        "# Save submission\n",
        "submission.to_csv('/content/improved_submission.csv', index=False)\n",
        "print(\"\\nSubmission file saved as 'improved_submission.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf-tvcTrm3i-",
        "outputId": "559a1c2c-e828-4ddf-f102-77c209bfffe7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Submission file saved as 'improved_submission.csv'\n"
          ]
        }
      ]
    }
  ]
}